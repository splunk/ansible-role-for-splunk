---
# This task MUST be called by configure_apps.yml to work correctly. Do NOT call this task directly via the deployment_task var!
- name: Set correct handler for master-apps
  set_fact:
    handler: "apply indexer cluster bundle"
  when: app_dest == 'etc/master-apps'

- name: Set correct handler for deployment-apps
  set_fact:
    handler: "reload deployment server"
  when: app_dest == 'etc/deployment-apps'

- name: Set correct handler for shcluster/apps
  set_fact:
    handler: "apply shcluster-bundle"
  when: app_dest == 'etc/shcluster/apps'

- name: "Set default restart splunk handler for all other paths (e.g. etc/auth)"
  set_fact:
    handler: "restart splunk"
  when:
    - app_dest != 'etc/shcluster/apps'
    - app_dest != 'etc/deployment-apps'
    - app_dest != 'etc/master-apps'

- name: Check if correct vars are defined for SHC app management
  block:
    - name: Trigger failure if target_shc_group_name var is undefined
      fail:
        msg: "Please add a target_shc_group_name variable to the host_vars for your SH Deployer host(s) before proceeding."
      when: target_shc_group_name is not defined

    - name: Trigger failure if splunk_admin_username var is undefined
      fail:
        msg: "Please add a splunk_admin_username variable to the host_vars for your SHC hosts before proceeding."
      when: splunk_admin_username is not defined

    - name: Trigger failure if splunk_admin_password var is undefined
      fail:
        msg:
          - "Please add a splunk_admin_password variable to the host_vars or group_vars for your SHC hosts before proceeding."
          - "Tip: To encrypt the var value, you can use: ansible-vault encrypt_string --ask-vault-pass 'var_value_to_encrypt' --name 'splunk_admin_password' then, use the --ask-vault-pass argument when running the play."
      when: splunk_admin_password is not defined

    - name: Get SHC status from first SH under the "target_shc_group_name" inventory group specified in the shdeployer host_vars
      uri:
        url: "https://{{ groups[target_shc_group_name] | first }}:{{ splunkd_port }}/services/shcluster/status"
        method: GET
        user: "{{ splunk_admin_username }}"
        password: "{{ splunk_admin_password }}"
        validate_certs: false
        return_content: true
        status_code: 200
        body:
          output_mode=json
      no_log: true
      changed_when: false
      register: shc_status_response

    - name: Set vars for deployment based on SHC status
      set_fact:
        service_ready_state: "{{ shc_status_response.json.entry[0].content.captain.service_ready_flag }}"
        deploy_target: "{{ shc_status_response.json.entry[0].content.captain.mgmt_uri }}"

    - name: Trigger failure if SHC is NOT ready
      fail:
        msg: "The SHC has service ready state of FALSE. Please investigate and re-run Ansible play after resolving."
      when:
        - not service_ready_state

  when: handler == "apply shcluster-bundle"

# Note: By using the synchronize module, if the repo already exists on the target host, we are able to only update the diff while preserving the local/ folder
- name: "Synchronize {{ item.name }} repo from local Ansible host to {{ splunk_home }}/{{ app_dest }}/{{ item.name }} on remote host"
  synchronize:
    src: "{{ app_src }}"
    dest: "{{ splunk_home }}/{{ app_dest }}/"
    recursive: true
    delete: true
    checksum: true
    rsync_opts:
      - "--prune-empty-dirs"
      - "--itemize-changes"
      - "--no-owner"
      - "--no-group"
      - "--no-times"
  become: true
  become_user: "{{ splunk_nix_user }}"
  notify: "{{ handler }}"

- name: Ensure correct permissions are set
  file:
    path: "{{ splunk_home }}/{{ app_dest }}/{{ item.name }}"
    owner: "{{ splunk_nix_user }}"
    group: "{{ splunk_nix_group }}"
    recurse: true
  become: true
